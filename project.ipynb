{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently Bought Toghether Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizializzazione, parametri e altre cose che tutti devono sapere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "import node2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Nodo target\n",
    "target = 3\n",
    "\n",
    "#Grafo completo FBT\n",
    "full_graph = nx.Graph()\n",
    "\n",
    "#Grafo cluster selezionato\n",
    "clustered_graph = nx.Graph()\n",
    "\n",
    "#File di lettura\n",
    "file_name = \"Amazon0302.txt\"\n",
    "#file_name = \"grafo_esempio.txt\"\n",
    "\n",
    "#Grafo directed se True o uniderected se False\n",
    "directed = True\n",
    "\n",
    "#embedding hyperparameters\n",
    "#default = 1, 1, 10, 80\n",
    "p=1\n",
    "q=1\n",
    "num_walks=10\n",
    "walk_length=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD GRAPH ###\n",
    "print(\"loading graph \" + file_name)\n",
    "\n",
    "if directed:\n",
    "    ##genera il grafo directed utilizzando nx.DiGraph\n",
    "    full_graph = nx.read_edgelist(file_name, nodetype=int, create_using=nx.DiGraph)\n",
    "\t\t\n",
    "else:\n",
    "    ##genera il grafo undirected utilizzando nx.Graph\n",
    "    full_graph = nx.read_edgelist(file_name)\n",
    "    \n",
    "nx.set_edge_attributes(full_graph, 1, name='weight')\n",
    "print(full_graph)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding using stanford's node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = node2vec.Graph(full_graph, directed, p, q)\n",
    "G.preprocess_transition_probs()\n",
    "walks = G.simulate_walks(num_walks, walk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = [list(map(str, walk)) for walk in walks]\n",
    "model = Word2Vec(walks, window=10, min_count=0, sg=1, workers=8)\n",
    "#model.save_word2vec_format(\"out.emb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "vector = model.wv[1]\n",
    "print(vector)\n",
    "\n",
    "NUMBER_OF_ITERATIONS = 10\n",
    "\n",
    "#k_means_cluster\n",
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.vector = x\n",
    "        self.assigned = y\n",
    "\n",
    "#calculate distance between two points\n",
    "def distance(point1, point2):\n",
    "    dist = 0\n",
    "    for i in range(point1.vector.len()):\n",
    "        dist += (point1.vector[i] - point2.vector[i])**2\n",
    "    return dist\n",
    "\n",
    "#calculate S score of point given array of centers and other points\n",
    "def calculate_silhouette(point, centers, points):\n",
    "    a_i = 0.0\n",
    "    b_i = float('inf')\n",
    "    cluster_size = 0\n",
    "    members = 0\n",
    "\n",
    "    for i in range(points.len()):\n",
    "        if points[i].assigned == point.assigned:\n",
    "            a_i += distance(point, points[i])\n",
    "            members += 1\n",
    "\n",
    "    if members < 2:\n",
    "        return 0\n",
    "    else:\n",
    "        a_i/= (members-1)\n",
    "\n",
    "    for k in range(centers.len()):\n",
    "        if not(k == point.assigned):\n",
    "            sum = 0.0\n",
    "            number = 0\n",
    "            for i in range(points.len()):\n",
    "                if points[i].assigned == k:\n",
    "                    sum += distance(point, points[i])\n",
    "                    number +=1\n",
    "            if not(number == 0):\n",
    "                sum /= number\n",
    "                if sum < b_i:\n",
    "                    b_i = sum\n",
    "    \n",
    "    if centers.len() == 1:\n",
    "        b_i = 0\n",
    "\n",
    "    return (b_i - a_i)/max(a_i, b_i)\n",
    "\n",
    "# calculate total silhouette score for clustering\n",
    "def calculate_silhouette_score(points, centers):\n",
    "    ss = 0.0\n",
    "    for point in points:\n",
    "        best_assign = -1\n",
    "        min_dist = float('inf')\n",
    "        for k in range(centers.len()):\n",
    "            if distance(point, centers[k] < min_dist):\n",
    "                point.assigned = k\n",
    "                min_dist = ditance(point, centers[k])\n",
    "\n",
    "    for p in points:\n",
    "        ss += calculate_silhouette(p, centers, points)\n",
    "        \n",
    "    return ss/points.size()\n",
    "\n",
    "def k_means_plus_plus(points, k_chosen):\n",
    "    ##init with kmeans++##\n",
    "    centroids = []\n",
    "    distances = []\n",
    "    probabilities = []\n",
    "\n",
    "    random_first_point = randrange(points.len())\n",
    "    centroids.append(points[random_first_point])\n",
    "    sum = 0.0\n",
    "    for p in points:\n",
    "        current = distance(centroids[0], p)\n",
    "        distances.append(current)\n",
    "        sum += current**2\n",
    "        probabilities.append(sum)\n",
    "    #choose a point with weighted probability for kmeans ++\n",
    "    for i in range(k_chosen):\n",
    "        if not(i == 0):\n",
    "            chosen = uniform(0.0, sum)\n",
    "            c = 0\n",
    "            for j in range(points.len()):\n",
    "                if chosen >= probabilities[j]:\n",
    "                    c = j\n",
    "\n",
    "        centroids.append(points[c])\n",
    "        sum = 0\n",
    "        for j in range(points.len()):\n",
    "            current = distance(centroids[i], points [j])\n",
    "            if(distances[j] > current):\n",
    "                distances[j] = current\n",
    "                points[j].assigned = i\n",
    "            sum += distances[j]**2\n",
    "            probabilities[j] = sum\n",
    "\n",
    "    ##start kmeans##\n",
    "    for iter in range(NUMBER_OF_ITERATIONS):\n",
    "        changed = False\n",
    "        for k in range(k_chosen):\n",
    "            med_vec = []\n",
    "            for j in range(points[0].vector.len()):\n",
    "                med = 0\n",
    "                pts = 0\n",
    "                for p in points:\n",
    "                    if(p.assigned == k):\n",
    "                        med += p.vector[j]\n",
    "                        pts += 1\n",
    "                med /= pts\n",
    "                med_vec.append(med)\n",
    "            centroids[k] = Point(med_vec, k)\n",
    "\n",
    "        for p in points:\n",
    "            best_dist = float('inf')\n",
    "            best_k = 0\n",
    "            for k in k_chosen:\n",
    "                dist = distance(p, centroids[k])\n",
    "                if(dist < best_dist):\n",
    "                    best_dist = dist\n",
    "                    best_k = k\n",
    "            if not(best_k == p.assigned):\n",
    "                changed = True\n",
    "                p.assigned = best_k\n",
    "                \n",
    "        if not changed:\n",
    "            break\n",
    "        else\n",
    "            changed = False\n",
    "\n",
    "    #find closest point to centroid\n",
    "    for k in range(k_chosen):\n",
    "        dist = float('inf')\n",
    "        i = -1\n",
    "        for p in range(point.len()):\n",
    "            dist_p = distance(points[p], centroids[k])\n",
    "            if dist_p < dist :\n",
    "                dist = dist_p\n",
    "                i = p\n",
    "        centroids[k] = points[i]\n",
    "        \n",
    "    return centroids\n",
    "\n",
    "def find_best_centroids(points):\n",
    "    best_centroids = []\n",
    "    best_score = -1.0\n",
    "    for k in range(points.len()).pop(0):\n",
    "        current_score = -1\n",
    "        current_centroids = k_means_plus_plus(points, k)\n",
    "        current_score = calculate_silhouette_score(points, current_centroids)\n",
    "        if(current_score >= best_score):\n",
    "            best_centroids = current_centroids\n",
    "            best_score = current_score\n",
    "        else\n",
    "            return best_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ = []\n",
    "#initialize vector of points to be used in Kmeans++\n",
    "for p in range(full_graph.number_of_nodes()):\n",
    "    points_.append(Point(model.wv[p], -1))\n",
    "\n",
    "best_c = find_best_centroids(points_)\n",
    "clustered = []\n",
    "\n",
    "for p in points_:\n",
    "    min_dist = float('inf')\n",
    "    for k in range(best_c.len()):\n",
    "        distance = distance(p, best_c[k])\n",
    "        if(distance < min_dist):\n",
    "            min_dist = distance\n",
    "            p.assigned = k\n",
    "            \n",
    "target_cluster = points_[target].assigned\n",
    "\n",
    "for p in points_:\n",
    "    if p.assigned == target_cluster:\n",
    "        clustered.append(p)\n",
    "\n",
    "clustered_graph = full_graph.subgraph(clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering global cc\n",
    "average_cluster_coef = nx.average_clustering(clustered_graph)\n",
    "\n",
    "print(f\"Coefficiente di clustering medio: {average_cluster_coef}\")\n",
    "\n",
    "\n",
    "\n",
    "#Clustering per ogni nodo (restituisce un dizionario)\n",
    "node_cluster_coefs = nx.clustering(clustered_graph)\n",
    "\n",
    "for node, cluster_coef in node_cluster_coefs.items():\n",
    "    print(f\"Nodo {node}: Coefficiente di clustering = {cluster_coef}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valutazione nodi (usare clustered_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valutazione nodi con CC o altre metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ricerca cliques\n",
    "\n",
    "#lettura file di esempio, costruzione nodes e edges\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "current_directory = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "filename = \"grafo_esempio.txt\"    \n",
    "\n",
    "graph = nx.Graph()\n",
    "graph = nx.read_edgelist(filename, nodetype=int, create_using=nx.DiGraph)\n",
    "nx.set_edge_attributes(graph, 1, name='weight')\n",
    "print(graph)\n",
    "\n",
    "print(\"Edges:\", graph.edges)\n",
    "print(\"Unique Node IDs:\", graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per il neighbor\n",
    "def n(v, edges):\n",
    "    neighbors = set()\n",
    "    for edge in edges:\n",
    "        if v in edge:\n",
    "            neighbors.update(edge)\n",
    "\n",
    "    neighbors.discard(v)\n",
    "\n",
    "    return list(neighbors)\n",
    "\n",
    "#Bron-Kerbosch algorithm with pivot\n",
    "def BronKerbosch(R, P, X, edges, cliques):\n",
    "    if not P and not X:\n",
    "        # P and X are both empty, report R as a maximal clique\n",
    "        cliques.append(R)\n",
    "        return\n",
    "\n",
    "    # Choose a pivot vertex u in P ⋃ X\n",
    "    pivot = (set(P) | set(X)).pop()\n",
    "\n",
    "    for v in set(P) - set(n(pivot, edges)):\n",
    "        # Recursively explore the neighborhood of v\n",
    "        BronKerbosch(R + [v], list(set(P) & set(n(v, edges))), list(set(X) & set(n(v, edges))), edges, cliques)\n",
    "\n",
    "        # Remove v from P and add it to X\n",
    "        P.remove(v)\n",
    "        X.append(v)\n",
    "\n",
    "#funzione per la clique massima\n",
    "def maxClique(cliques):\n",
    "    return max(cliques, key=len, default=[])\n",
    "\n",
    "#Ricerca e stampa le cliques e la clique massima\n",
    "all_cliques = []\n",
    "node_cliques = []\n",
    "selected_node = 10\n",
    "BronKerbosch([], list(graph.nodes), [], graph.edges, all_cliques)\n",
    "print(\"Cliques:\")\n",
    "for clique in all_cliques:\n",
    "    if selected_node in clique:\n",
    "        node_cliques.append(clique)\n",
    "        \n",
    "print(node_cliques)\n",
    "\n",
    "print(\"Maximum Clique:\", maxClique(node_cliques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering global cc\n",
    "average_cluster_coef = nx.average_clustering(graph)\n",
    "\n",
    "print(f\"Coefficiente di clustering medio: {average_cluster_coef}\")\n",
    "\n",
    "\n",
    "\n",
    "#Clustering per ogni nodo (restituisce un dizionario)\n",
    "node_cluster_coefs = nx.clustering(graph)\n",
    "\n",
    "for node, cluster_coef in node_cluster_coefs.items():\n",
    "    print(f\"Nodo {node}: Coefficiente di clustering = {cluster_coef}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node clique with max cc\n",
    "max_cc = 0\n",
    "best_clique = []\n",
    "for clique in node_cliques:\n",
    "    avg_cc = 0;\n",
    "    for node in clique:\n",
    "        avg_cc += node_cluster_coefs[node]\n",
    "        \n",
    "    if avg_cc/len(clique) > max_cc:\n",
    "        best_clique = clique\n",
    "        \n",
    "print(best_clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tentativi con rimozioni casuali\n",
    "import random\n",
    "\n",
    "# Numero di nodi da rimuovere ad ogni iterazione\n",
    "n_nodes_to_remove = 1\n",
    "\n",
    "# Numero totale di iterazioni\n",
    "n_iterations = 10\n",
    "\n",
    "# Copia del grafo iniziale\n",
    "graph_to_modify = full_graph.copy()\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Rimuovi n nodi casuali dal grafo\n",
    "    nodes_to_remove = random.sample(list(graph_to_modify.nodes()), n_nodes_to_remove)\n",
    "    updated_graph = graph_to_modify.copy()\n",
    "    updated_graph.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "    # Stampa o elabora i risultati desiderati\n",
    "    print(f\"Iteration {iteration + 1}: Removed nodes {nodes_to_remove}\")\n",
    "    \n",
    "    # Aggiorna il grafo originale per la prossima iterazione\n",
    "    original_graph = updated_graph.copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
